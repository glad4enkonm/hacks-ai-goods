from prompt_template_utils import get_prompt_template
import torch
import streamlit as st
from run_localGPT import load_model
from langchain.callbacks.base import BaseCallbackHandler

equipment_question="–í—ã–≤–µ–¥–∏—Ç–µ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Ç–æ–ª—å–∫–æ –∏–∑–º–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ (–∞–ø–ø–∞—Ä–∞—Ç—É—Ä—É, –ø—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–∏—è, –ø—Ä–∏–±–æ—Ä—ã, –º–∞—à–∏–Ω—ã), –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–µ –≤ Context, —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ä–∞–∑–¥–µ–ª–∞ –∏–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞, –≤ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω–æ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 4.1). –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –æ—Ç–≤–µ—Ç—å—Ç–µ –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π."
promptOuter,_ = get_prompt_template()

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text="", display_method='markdown'):
        self.container = container
        self.text = initial_text
        self.display_method = display_method

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token + "/"
        display_function = getattr(self.container, self.display_method, None)
        if display_function is not None:
            display_function(self.text)
        else:
            raise ValueError(f"Invalid display_method: {self.display_method}")

# Sidebar contents
with st.sidebar:
    st.title("üí¨ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ ")
    st.markdown(
        """
    ## –ò–Ω—Ñ–æ
    –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞:
    - [Streamlit](https://streamlit.io/)
    - [LangChain](https://python.langchain.com/)
    - [LocalGPT](https://github.com/PromtEngineer/localGPT)
    """
    )


if torch.cuda.is_available():
    DEVICE_TYPE = "cuda"
else:
    DEVICE_TYPE = "cpu"

chat_box = st.empty()
stream_handler = StreamHandler(chat_box, display_method='write')

if "LLM" not in st.session_state:
    LLM = load_model(stream_handler)
    st.session_state["LLM"] = LLM

st.title("–î–æ–∫—É–º–µ–Ω—Ç—ã –±–ª–∏–∂–µ üí¨")
# Create a text input box for the user
prompt = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º", max_chars=2000, height=400)

# If the user hits enter
if prompt:
    # Then pass the prompt to the LLM
    response = st.session_state["LLM"](prompt=promptOuter.format(context=prompt, question=equipment_question))
    # ...and write it out to the screen
    st.write(response)
